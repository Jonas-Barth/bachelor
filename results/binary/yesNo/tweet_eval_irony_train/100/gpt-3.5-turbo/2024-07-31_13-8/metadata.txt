Model used: gpt-3.5-turbo
Prompt: You are an irony detector. Respond with 'Yes' or 'No' depending on whether you think the following statements are ironic.

Alternate Prompt (prompt engineering): yesNo
Dataset: datasets\tweet_eval_irony_train.csv
Amount of individual evaluations (sample size): 100

Results for run 1: 
Matrix:
31  9
18  42
Accuracy: 0.73
Precision: 0.775
Recall: 0.6326530612244898
F1-Score: 0.6966292134831462

Errors (not parsed): 


Results for run 2: 
Matrix:
31  16
18  35
Accuracy: 0.66
Precision: 0.6595744680851063
Recall: 0.6326530612244898
F1-Score: 0.6458333333333333

Errors (not parsed): 


Results for run 3: 
Matrix:
23  12
26  39
Accuracy: 0.62
Precision: 0.6571428571428571
Recall: 0.46938775510204084
F1-Score: 0.5476190476190477

Errors (not parsed): 


Results for run 4: 
Matrix:
23  15
26  36
Accuracy: 0.59
Precision: 0.6052631578947368
Recall: 0.46938775510204084
F1-Score: 0.5287356321839081

Errors (not parsed): 


Results for run 5: 
Matrix:
30  12
19  39
Accuracy: 0.69
Precision: 0.7142857142857143
Recall: 0.6122448979591837
F1-Score: 0.6593406593406593

Errors (not parsed): 


Results for run 6: 
Matrix:
27  15
22  36
Accuracy: 0.63
Precision: 0.6428571428571429
Recall: 0.5510204081632653
F1-Score: 0.5934065934065934

Errors (not parsed): 


Results for run 7: 
Matrix:
28  12
21  39
Accuracy: 0.67
Precision: 0.7
Recall: 0.5714285714285714
F1-Score: 0.6292134831460674

Errors (not parsed): 


Results for run 8: 
Matrix:
31  12
18  39
Accuracy: 0.7
Precision: 0.7209302325581395
Recall: 0.6326530612244898
F1-Score: 0.6739130434782609

Errors (not parsed): 


Results for run 9: 
Matrix:
31  14
18  37
Accuracy: 0.68
Precision: 0.6888888888888889
Recall: 0.6326530612244898
F1-Score: 0.6595744680851063

Errors (not parsed): 


Results for run 10: 
Matrix:
33  12
16  39
Accuracy: 0.72
Precision: 0.7333333333333333
Recall: 0.673469387755102
F1-Score: 0.702127659574468

Errors (not parsed): 


Average F1 score across 10 runs: 0.633639313365059