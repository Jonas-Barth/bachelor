{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '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'   #  OLD KEY\n",
    "#key = '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'\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET FROM ONLINE\n",
    "\n",
    "#from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "\n",
    "#for index, row in dataset.iterrows():\n",
    "#    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gptNoHistory (dataset, sysprompt, modelName):\n",
    "    client = OpenAI(api_key = key)\n",
    "    #results = {}\n",
    "    content = []\n",
    "    resultEval = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        startmsg = [{\"role\": \"system\", \"content\": sysprompt}, {\"role\": \"user\", \"content\": row[0]}]\n",
    "        chat_completion = client.chat.completions.create(messages = startmsg, model = modelName)\n",
    "        if ((index + 1) % 20 == 0):\n",
    "            print(index + 1, ((index + 1)/len(dataset)) * 100, \"%\")\n",
    "        #results[row[0]] = chat_completion.choices[0].message.content\n",
    "        content.append(row[0])\n",
    "        resultEval.append(chat_completion.choices[0].message.content)\n",
    "    \n",
    "    resultData = {\n",
    "        'content': content,\n",
    "        'classification': resultEval,\n",
    "        'model': chat_completion.model\n",
    "    }\n",
    "    results = pd.DataFrame(resultData)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gptAreYouSure (dataset, sysprompt, modelName):\n",
    "    client = OpenAI(api_key = key)\n",
    "    resultSure = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        message_history = [{\"role\": \"system\", \"content\": sysprompt}]\n",
    "        message_history.append({\"role\": \"user\", \"content\": row[0]})\n",
    "        chat_completion = client.chat.completions.create(messages = message_history, model = modelName)\n",
    "        resultString = chat_completion.choices[0].message.content\n",
    "        message_history.append({\"role\": chat_completion.choices[0].message.role, \"content\": chat_completion.choices[0].message.content})\n",
    "        message_history.append({\"role\": \"user\", \"content\": \"Are you sure? Answer with 'Yes' or 'No'.\"})\n",
    "        chat_completion = client.chat.completions.create(messages = message_history, model = modelName)\n",
    "        resultString = resultString + ' ' + chat_completion.choices[0].message.content\n",
    "        if ((index + 1) % 20 == 0):\n",
    "            print('Progress:', index + 1, ((index + 1)/len(dataset)) * 100, \"%\")\n",
    "        resultSure.append(resultString)\n",
    "    return resultSure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE AS TOKEN LENGTH BECOMES TOO LARGE AND MAKES THE KEY RUN OUT OF AVAILABLE TOKENS FAST\n",
    "\n",
    "\n",
    "#def gptWithHistory (dataset, sysprompt, modelName):\n",
    "#    client = OpenAI(api_key = key)\n",
    "\n",
    "#    results = {}\n",
    "#    message_history = [{\"role\": \"system\", \"content\": sysprompt}]\n",
    "\n",
    "#    for index, row in dataset.iterrows():\n",
    "#        message_history.append({\"role\": \"user\", \"content\": row[0]}) # add tweet to the messages\n",
    "#        chat_completion = client.chat.completions.create(messages = message_history, model = modelName)\n",
    "#        message_history.append({\"role\": chat_completion.choices[0].message.role, \"content\": chat_completion.choices[0].message.content})\n",
    "#        if ((index + 1) % 20 == 0):\n",
    "#            print('Progress:', index + 1, ((index + 1)/len(dataset)) * 100, \"%\")\n",
    "#        results[row[0]] = chat_completion.choices[0].message.content\n",
    "#    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFScore(truepos, falsepos, falseneg):\n",
    "    FScoreResults = {}\n",
    "    FScoreResults['precision'] = truepos/(truepos + falsepos)\n",
    "    FScoreResults['recall'] = truepos/(truepos + falseneg)\n",
    "    FScoreResults['F1'] = (2 * FScoreResults['precision'] * FScoreResults['recall'])/(FScoreResults['precision'] + FScoreResults['recall'])\n",
    "    return FScoreResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def matrixPlot (tp, fp, fn, tn, path, runNum):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    #tp = np.array([10, 50 ,40])\n",
    "    #fp = np.array([50, 40, 30])\n",
    "    #fn = np.array([80, 90, 60])\n",
    "    #tn = np.array([80, 90, 60])\n",
    "\n",
    "    tp_mean = np.mean(tp)\n",
    "    fp_mean = np.mean(fp)\n",
    "    fn_mean = np.mean(fn)\n",
    "    tn_mean = np.mean(tn)\n",
    "\n",
    "    tp_std = np.std(tp)\n",
    "    fp_std = np.std(fp)\n",
    "    fn_std = np.std(fn)\n",
    "    tn_std = np.std(tn)\n",
    "\n",
    "    labels = ['True Positive (' + str(len(tp)) + ')', 'False Positive (' + str(len(fp)) + ')', 'False Negative (' + str(len(fn)) + ')', 'True Negative (' + str(len(tn)) + ')']\n",
    "    x_pos = np.arange(len(labels))\n",
    "    CTEs = [tp_mean, fp_mean, fn_mean, tn_mean]\n",
    "    error = [tp_std, fp_std, fn_std, tn_std]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x_pos, CTEs, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "    ax.set_ylabel('Degree of confidence in evaluation')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title('Confidence comparison separated into correctness')\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + '\\\\Figure Run ' + str(runNum + 1))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def matrixPlotSure(tp, tps, fp, fps, fn, fns, tn, tns, path, runNum):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    #tp = np.array([10, 50 ,40])\n",
    "    #fp = np.array([50, 40, 30])\n",
    "    #fn = np.array([80, 90, 60])\n",
    "    #tn = np.array([80, 90, 60])\n",
    "\n",
    "    labels = ['True Pos (' + str(tps) + '/' + str(tp - tps) + ')', 'False Pos (' + str(fps) + '/' + str(fp - fps) + ')', 'False Neg (' + str(fns) + '/' + str(fn - fns) + ')', 'True Neg (' + str(tns) + '/' + str(tn - tns) + ')']\n",
    "    x_pos = np.arange(len(labels))\n",
    "    CTEs = [tp, fp, fn, tn]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x_pos, CTEs, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "    ax.set_ylabel('Amount of answers per categorization')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title('Answers by binary confidence measure (Sure? Yes/No in parentheses)')\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.ylim(0, (tp + fp + fn + tn))\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + '\\\\Figure Run ' + str(runNum + 1))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a binary classification of irony\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresBinary(resultSetScores, gptcolumn):\n",
    "    truepos = 0\n",
    "    falsepos = 0\n",
    "    trueneg = 0\n",
    "    falseneg = 0\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        if (row[gptcolumn] == '1' or row[gptcolumn] == '0'):\n",
    "            if (int(row[1]) == int(row[gptcolumn])):\n",
    "                if (int(row[1]) == 1):\n",
    "                    truepos = truepos + 1\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "            elif (int(row[1]) == 0):\n",
    "                falsepos = falsepos + 1\n",
    "            elif (int(row[1]) == 1):\n",
    "                falseneg = falseneg + 1\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    # matrix\n",
    "    #print(truepos, falsepos)\n",
    "    #print(falseneg, trueneg)\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a binary classification of irony\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresBinaryYesNo(resultSetScores, gptcolumn):\n",
    "    truepos = 0\n",
    "    falsepos = 0\n",
    "    trueneg = 0\n",
    "    falseneg = 0\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        stringYN = row[gptcolumn].replace('.', '') # Turns \"Yes.\" or \"No.\" answers into \"Yes\" or \"No\".\n",
    "        if (stringYN == 'Yes' or stringYN == 'No'):\n",
    "            if (int(row[1]) == 1):\n",
    "                if (stringYN == 'Yes'):\n",
    "                    truepos = truepos + 1\n",
    "                else:\n",
    "                    falseneg = falseneg + 1\n",
    "            elif (int(row[1]) == 0): # usually always the case but just checking to be sure\n",
    "                if (stringYN == 'Yes'):\n",
    "                    falsepos = falsepos + 1\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    # matrix\n",
    "    #print(truepos, falsepos)\n",
    "    #print(falseneg, trueneg)\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a binary classification of irony and an added confidence measure.\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresBinaryConf(resultSetScores, gptcolumn):\n",
    "    import re\n",
    "\n",
    "    truepos = 0\n",
    "    trueposConf = []\n",
    "    falsepos = 0\n",
    "    falseposConf = []\n",
    "    trueneg = 0\n",
    "    truenegConf = []\n",
    "    falseneg = 0\n",
    "    falsenegConf = []\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        if (re.match(r'(1|0)\\s((10(0)?)|(\\d(\\d)?))(%?)', row[gptcolumn])):\n",
    "            if (int(row[1]) == int(row[gptcolumn][0])):\n",
    "                if (int(row[1]) == 1):\n",
    "                    truepos = truepos + 1\n",
    "                    trueposConf = trueposConf + [int(row[gptcolumn][2:4])]\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "                    truenegConf = truenegConf + [int(row[gptcolumn][2:4])]\n",
    "            elif (int(row[1]) == 0):\n",
    "                falsepos = falsepos + 1\n",
    "                falseposConf = falseposConf + [int(row[gptcolumn][2:4])]\n",
    "            elif (int(row[1]) == 1):\n",
    "                falseneg = falseneg + 1\n",
    "                falsenegConf = falsenegConf + [int(row[gptcolumn][2:4])]\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    # matrix\n",
    "    #print(truepos, falsepos)\n",
    "    #print(falseneg, trueneg)\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['tpConf'] = trueposConf\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fpConf'] = falseposConf\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['fnConf'] = falsenegConf\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['tnConf'] = truenegConf\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a binary classification of irony and an added evaluation of whether gpt is sure about its evaluation.\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresBinarySure(resultSetScores, gptcolumn):\n",
    "    import re\n",
    "\n",
    "    truepos = 0\n",
    "    trueposSure = 0\n",
    "    falsepos = 0\n",
    "    falseposSure = 0\n",
    "    trueneg = 0\n",
    "    truenegSure = 0\n",
    "    falseneg = 0\n",
    "    falsenegSure = 0\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        if (re.match(r'\\b(1|0)\\s(Yes|No)\\b', row[gptcolumn])):\n",
    "            if (int(row[1]) == int(row[gptcolumn][0])):\n",
    "                if (int(row[1]) == 1):\n",
    "                    truepos = truepos + 1\n",
    "                    if (row[gptcolumn][2:5] == 'Yes'):\n",
    "                        trueposSure = trueposSure + 1\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "                    if (row[gptcolumn][2:5] == 'Yes'):\n",
    "                        truenegSure = truenegSure + 1\n",
    "            elif (int(row[1]) == 0):\n",
    "                falsepos = falsepos + 1\n",
    "                if (row[gptcolumn][2:5] == 'Yes'):\n",
    "                    falseposSure = falseposSure + 1\n",
    "            elif (int(row[1]) == 1):\n",
    "                falseneg = falseneg + 1\n",
    "                if (row[gptcolumn][2:5] == 'Yes'):\n",
    "                    falsenegSure = falsenegSure + 1\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    # matrix\n",
    "    #print(truepos, falsepos)\n",
    "    #print(falseneg, trueneg)\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['tpSure'] = trueposSure\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fpSure'] = falseposSure\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['fnSure'] = falsenegSure\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['tnSure'] = truenegSure\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a binary classification of irony\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresSentChoice(resultSetScores, gptcolumn):\n",
    "    truepos = 0\n",
    "    falsepos = 0\n",
    "    trueneg = 0\n",
    "    falseneg = 0\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        string = row[gptcolumn].replace('.', '') # removes possible periods at the end for more unified answer format\n",
    "        if (string.lower() in ['angry', 'sad', 'ironic', 'happy']):\n",
    "            if (int(row[1]) == 1):\n",
    "                if (string == 'ironic'):\n",
    "                    truepos = truepos + 1\n",
    "                else:\n",
    "                    falseneg = falseneg + 1\n",
    "            elif (int(row[1]) == 0): # usually always the case but just checking to be sure\n",
    "                if (string == 'ironic'):\n",
    "                    falsepos = falsepos + 1\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    # matrix\n",
    "    #print(truepos, falsepos)\n",
    "    #print(falseneg, trueneg)\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a percentage classification of irony.\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresPercentage(resultSetScores, gptcolumn):\n",
    "    import re\n",
    "\n",
    "    truepos = 0\n",
    "    falsepos = 0\n",
    "    trueneg = 0\n",
    "    falseneg = 0\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        string = row[gptcolumn]\n",
    "        string = string[0:3]\n",
    "        string = string.replace(' ', '')\n",
    "        #print('-' + string + '-')\n",
    "        if (re.match(r'(\\d{1,2}%?|(100(%?)))$', string)):\n",
    "            if ('%' in string):\n",
    "                string = string[0:len(string) - 1] # remove % sign at end if applicable\n",
    "            result = 0\n",
    "            if (int(string) >= 50): # if an evaluation is considered by GPT to consist of 50% or more irony then the message is considered ironic for the purposes of this experiment.\n",
    "                result = 1\n",
    "            if (int(row[1]) == result):\n",
    "                if (int(row[1]) == 1):\n",
    "                    truepos = truepos + 1\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "            elif (int(row[1]) == 0):\n",
    "                falsepos = falsepos + 1\n",
    "            elif (int(row[1]) == 1):\n",
    "                falseneg = falseneg + 1\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listAvg (numbers):\n",
    "    total_sum = sum(numbers)\n",
    "    count = len(numbers)\n",
    "    if count == 0:\n",
    "        average = 0\n",
    "    else:\n",
    "        average = total_sum / count\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 8\n",
      "-5%-\n"
     ]
    }
   ],
   "source": [
    "# SMALL TEST CELL TO TEST DATASETS MANUALLY (pre-gpt)\n",
    "dataset = pd.read_csv(\"datasets\\\\tweet_eval_irony_train.csv\")\n",
    "dataset = dataset.head(20)\n",
    "notIrony = 0\n",
    "irony = 0\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    if (row[1] == 1):\n",
    "        irony = irony + 1\n",
    "    else:\n",
    "        notIrony = notIrony + 1\n",
    "\n",
    "print(irony, notIrony)\n",
    "\n",
    "strTest = '5% '\n",
    "strTest = strTest.replace(\" \", \"\")\n",
    "print('-' + strTest + '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores controller\n",
    "def scoresController(resultSetScores, gptcolumn, type):\n",
    "    if (type == 'binary'):\n",
    "        return scoresBinary(resultSetScores, gptcolumn)\n",
    "    elif(type == 'binaryYesNo'):\n",
    "        return scoresBinaryYesNo(resultSetScores, gptcolumn)\n",
    "    elif(type == 'confidence'):\n",
    "        return scoresBinaryConf(resultSetScores, gptcolumn)\n",
    "    elif(type == 'sure'):\n",
    "        return scoresBinarySure(resultSetScores, gptcolumn)\n",
    "    elif(type == 'sentimentchoice'):\n",
    "        return scoresSentChoice(resultSetScores, gptcolumn)\n",
    "    elif(type == 'percentage'):\n",
    "        return scoresPercentage(resultSetScores, gptcolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Runtype Controller\n",
    "def gptRunType(data, systemPrompt, model, type):\n",
    "    if (type == 'sure'):\n",
    "        return gptAreYouSure(data, systemPrompt, model)\n",
    "    else:\n",
    "        return gptNoHistory(data, systemPrompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionsForRunType(linesWrite, result, type, resultRunNum, path):\n",
    "    if(type == 'confidence'):\n",
    "        linesWrite = linesWrite + [\n",
    "        'truepos confidence avg: ' + str(listAvg(result['tpConf'])) + '\\n',\n",
    "        'falsepos confidence avg: ' + str(listAvg(result['fpConf'])) + '\\n',\n",
    "        'falseneg confidence avg: ' + str(listAvg(result['fnConf'])) + '\\n',\n",
    "        'trueneg confidence avg: ' + str(listAvg(result['tnConf'])) + '\\n']\n",
    "        matrixPlot(result['tpConf'], result['fpConf'], result['fnConf'], result['tnConf'], path + '\\\\figures', resultRunNum)\n",
    "        return linesWrite\n",
    "    elif(type == 'sure'):\n",
    "        linesWrite = linesWrite + ['Sure about ' + str(result['tpSure'])  + ' true positives, ' + str(result['fpSure']) + ' false positives, ' + str(result['fnSure']) + ' false negatives, and ' + str(result['tnSure']) + ' true negatives.\\n']\n",
    "        matrixPlotSure(result['tp'], result['tpSure'], result['fp'], result['fpSure'], result['fn'], result['fnSure'], result['tn'], result['tnSure'], path + '\\\\figures', resultRunNum)\n",
    "        return linesWrite\n",
    "    else:\n",
    "        return linesWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if (re.match(r'(\\d{1,2}%?|(100(%?)))$', '0%')):\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20.0 %\n",
      "40 40.0 %\n",
      "60 60.0 %\n",
      "80 80.0 %\n",
      "100 100.0 %\n",
      "Run 1 done!\n",
      "20 20.0 %\n",
      "40 40.0 %\n",
      "60 60.0 %\n",
      "80 80.0 %\n",
      "100 100.0 %\n",
      "Run 2 done!\n",
      "20 20.0 %\n",
      "40 40.0 %\n",
      "60 60.0 %\n",
      "80 80.0 %\n",
      "100 100.0 %\n",
      "Run 3 done!\n",
      "20 20.0 %\n",
      "40 40.0 %\n",
      "60 60.0 %\n",
      "80 80.0 %\n",
      "100 100.0 %\n",
      "Run 4 done!\n",
      "20 20.0 %\n",
      "40 40.0 %\n",
      "60 60.0 %\n",
      "80 80.0 %\n",
      "100 100.0 %\n",
      "Run 5 done!\n"
     ]
    }
   ],
   "source": [
    "# GPT CONTROLLER\n",
    "# runtypes = binary, confidence, sure, sentimentchoice, percentage\n",
    "runType = 'binary'\n",
    "datasetName = \"tweet_eval_irony_train\"\n",
    "datasetPath = \"datasets\\\\\" + datasetName + \".csv\"\n",
    "data = pd.read_csv(datasetPath)\n",
    "data = data.head(100)\n",
    "\n",
    "amountOfRuns = 5\n",
    "\n",
    "alternate = 3 # prompt engineering: selects the prompt in the alternate + 1th line of the text file. Set to 0 if going with default prompt\n",
    "\n",
    "results = []\n",
    "\n",
    "promptFile = open(\"prompts\\\\\" + runType + \".txt\")\n",
    "\n",
    "systemPrompt = promptFile.readline()\n",
    "if(alternate > 0):\n",
    "    for i in range(0, alternate):\n",
    "       systemPrompt = promptFile.readline()\n",
    "\n",
    "altList = systemPrompt.split(';')\n",
    "alternateName = altList[0]\n",
    "systemPrompt = altList[1]\n",
    "\n",
    "#model = \"gpt-4\" #gpt-4o, gpt-4-turbo, gpt-4, and gpt-3.5-turbo\n",
    "model = \"gpt-4\"\n",
    "#model = \"gpt-3.5-turbo\"\n",
    "\n",
    "for x in range(amountOfRuns):\n",
    "    resultSet = gptRunType(data, systemPrompt, model, runType)\n",
    "    print('Run ' + str(x + 1) + ' done!')\n",
    "    results.append(resultSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# create dataframe that contains the original tweet (column 0), the original classification (column 1) and the gpt classification of the tweet (column 2)\n",
    "resultSetAlt = data\n",
    "runNo = 1\n",
    "resultScores = []\n",
    "\n",
    "# take original dataset, add new columns that give classification for that line as returned by gpt\n",
    "for x in results:\n",
    "    string = 'gpt run no. ' + str(runNo)\n",
    "    runNo = runNo + 1\n",
    "    if (runType == 'sure'):\n",
    "        resultSetAlt[string] = x\n",
    "    else:\n",
    "        resultSetAlt[string] = x['classification']\n",
    "\n",
    "# calculate and save scores\n",
    "for x in range(amountOfRuns):\n",
    "    res = scoresController(resultSetAlt, x + 2, runType)\n",
    "    resultScores.append(res)\n",
    "\n",
    "# create folder for dataset, current date and time to sort results\n",
    "now = datetime.datetime.now()\n",
    "pathTime = \"results\\\\\"+ runType + \"\\\\\" + str(alternateName) + \"\\\\\" + datasetName + '\\\\' +  str(len(resultSetAlt)) + '\\\\' + model + '\\\\' + str(now.date()) + \"_\" + str(now.time().hour) + \"-\" + str(now.time().minute)\n",
    "if not os.path.exists(pathTime):\n",
    "    os.makedirs(pathTime)\n",
    "\n",
    "# create text file containing the relevant results from the experiment\n",
    "\n",
    "linesToWrite = ['Model used: ' + model + '\\n',\n",
    "                #'Model (given by last run): ' + str(results[(amountOfRuns - 1)]['model']) + '\\n',\n",
    "                'Prompt: ' + systemPrompt + '\\n']\n",
    "if (alternate == 0):\n",
    "    linesToWrite = linesToWrite + ['Alternate Prompt (prompt engineering): No' + '\\n']\n",
    "else:\n",
    "    linesToWrite = linesToWrite + ['Alternate Prompt (prompt engineering): ' + str(alternateName) + '\\n']\n",
    "\n",
    "linesToWrite = linesToWrite + ['Dataset: ' + datasetPath + '\\n',\n",
    "                'Amount of individual evaluations (sample size): ' + str(len(resultSetAlt)) + '\\n\\n']\n",
    "\n",
    "resultRuns = 0\n",
    "averageF1 = 0.0\n",
    "for res in resultScores:\n",
    "    errorString = 'Errors (not parsed): \\n'\n",
    "    for error in res['error']:\n",
    "        errorString = errorString + error + '\\n'\n",
    "\n",
    "    linesToWrite = linesToWrite + ['Results for run ' + str(resultRuns + 1) + ': \\n',\n",
    "        'Matrix:' + '\\n',\n",
    "        str(res['tp']) + '  ' + str(res['fp']) + '\\n',\n",
    "        str(res['fn']) + '  ' + str(res['tn']) + '\\n']\n",
    "    linesToWrite = actionsForRunType(linesToWrite, res, runType, resultRuns, pathTime)\n",
    "    linesToWrite = linesToWrite + ['Precision: ' + str(res['precision']) + '\\n',\n",
    "        'Recall: ' + str(res['recall']) + '\\n',\n",
    "        'F1-Score: ' + str(res['F1']) + '\\n\\n',\n",
    "        errorString + '\\n\\n']\n",
    "    averageF1 = averageF1 + res['F1']\n",
    "    resultRuns = resultRuns + 1\n",
    "    \n",
    "averageF1 = averageF1/resultRuns\n",
    "\n",
    "linesToWrite = linesToWrite + ['Average F1 score across ' + str(resultRuns) + ' runs: ' + str(averageF1)]\n",
    "\n",
    "file = open(pathTime + \"\\\\metadata.txt\", \"w\")\n",
    "file.writelines(linesToWrite)\n",
    "file.close()\n",
    "\n",
    "# save the original data evaluated as well as the results (all of which is in resultSetAlt) as a csv for review if required\n",
    "resultSetAlt.to_csv(pathTime + '\\\\results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
