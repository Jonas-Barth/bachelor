{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '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'   #  OLD KEY\n",
    "#key = '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'\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET FROM ONLINE\n",
    "\n",
    "#from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "\n",
    "#for index, row in dataset.iterrows():\n",
    "#    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gptNoHistory (dataset, sysprompt, modelName):\n",
    "    client = OpenAI(api_key = key)\n",
    "    #results = {}\n",
    "    content = []\n",
    "    resultEval = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        startmsg = [{\"role\": \"system\", \"content\": sysprompt}, {\"role\": \"user\", \"content\": row[0]}]\n",
    "        chat_completion = client.chat.completions.create(messages = startmsg, model = modelName)\n",
    "        if ((index + 1) % 20 == 0):\n",
    "            print(index + 1, ((index + 1)/len(dataset)) * 100, \"%\")\n",
    "        #results[row[0]] = chat_completion.choices[0].message.content\n",
    "        content.append(row[0])\n",
    "        resultEval.append(chat_completion.choices[0].message.content)\n",
    "    \n",
    "    resultData = {\n",
    "        'content': content,\n",
    "        'classification': resultEval,\n",
    "        'model': chat_completion.model\n",
    "    }\n",
    "    results = pd.DataFrame(resultData)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE AS TOKEN LENGTH BECOMES TOO LARGE AND MAKES THE KEY RUN OUT OF AVAILABLE TOKENS FAST\n",
    "\n",
    "\n",
    "#def gptWithHistory (dataset, sysprompt, modelName):\n",
    "#    client = OpenAI(api_key = key)\n",
    "\n",
    "#    results = {}\n",
    "#    message_history = [{\"role\": \"system\", \"content\": sysprompt}]\n",
    "\n",
    "#    for index, row in dataset.iterrows():\n",
    "#        message_history.append({\"role\": \"user\", \"content\": row[0]}) # add tweet to the messages\n",
    "#        chat_completion = client.chat.completions.create(messages = message_history, model = modelName)\n",
    "#        message_history.append({\"role\": chat_completion.choices[0].message.role, \"content\": chat_completion.choices[0].message.content})\n",
    "#        if ((index + 1) % 20 == 0):\n",
    "#            print('Progress:', index + 1, ((index + 1)/len(dataset)) * 100, \"%\")\n",
    "#        results[row[0]] = chat_completion.choices[0].message.content\n",
    "#    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFScore(truepos, falsepos, falseneg):\n",
    "    FScoreResults = {}\n",
    "    FScoreResults['precision'] = truepos/(truepos + falsepos)\n",
    "    FScoreResults['recall'] = truepos/(truepos + falseneg)\n",
    "    FScoreResults['F1'] = (2 * FScoreResults['precision'] * FScoreResults['recall'])/(FScoreResults['precision'] + FScoreResults['recall'])\n",
    "    return FScoreResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate result scores using the answers from GPT for a binary classification of irony\n",
    "# generalized for any dataset, as long as the classification from the original dataset and gpt are the same (e.g., '1' for irony and '0' for non-irony)\n",
    "def scoresBinary(resultSetScores, gptcolumn):\n",
    "    truepos = 0\n",
    "    falsepos = 0\n",
    "    trueneg = 0\n",
    "    falseneg = 0\n",
    "    errors = []\n",
    "\n",
    "    for index, row in resultSetScores.iterrows():\n",
    "        if (row[gptcolumn] == '1' or row[gptcolumn] == '0'):\n",
    "            if (int(row[1]) == int(row[gptcolumn])):\n",
    "                if (int(row[1]) == 1):\n",
    "                    truepos = truepos + 1\n",
    "                else:\n",
    "                    trueneg = trueneg + 1\n",
    "            elif (int(row[1]) == 0):\n",
    "                falsepos = falsepos + 1\n",
    "            elif (int(row[1]) == 1):\n",
    "                falseneg = falseneg + 1\n",
    "        else:\n",
    "            print(\"Failure in line \" + str(index) + \" in gpt answer column \" + str(gptcolumn - 1) + ' (answer format not correct). Error line: ' + str(row[gptcolumn]))\n",
    "            errors.append(str(row[gptcolumn]))\n",
    "\n",
    "    # matrix\n",
    "    #print(truepos, falsepos)\n",
    "    #print(falseneg, trueneg)\n",
    "\n",
    "    numResults = calcFScore(truepos, falsepos, falseneg)\n",
    "    numResults['tp'] = truepos\n",
    "    numResults['fp'] = falsepos\n",
    "    numResults['fn'] = falseneg\n",
    "    numResults['tn'] = trueneg\n",
    "    numResults['error'] = errors\n",
    "    return numResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 8\n"
     ]
    }
   ],
   "source": [
    "# SMALL TEST CELL TO TEST DATASETS MANUALLY (pre-gpt)\n",
    "dataset = pd.read_csv(\"datasets\\\\tweet_eval_irony_train.csv\")\n",
    "dataset = dataset.head(20)\n",
    "notIrony = 0\n",
    "irony = 0\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    if (row[1] == 1):\n",
    "        irony = irony + 1\n",
    "    else:\n",
    "        notIrony = notIrony + 1\n",
    "\n",
    "print(irony, notIrony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20.0 %\n",
      "40 40.0 %\n",
      "60 60.0 %\n"
     ]
    }
   ],
   "source": [
    "# GPT CONTROLLER\n",
    "# runtypes = binary, binaryyesno, confidence, sure\n",
    "runtype = 'binary'\n",
    "datasetName = \"fixedsetreadin\"\n",
    "datasetPath = \"datasets\\\\\" + datasetName + \".csv\"\n",
    "data = pd.read_csv(datasetPath)\n",
    "data = data.head(100)\n",
    "data\n",
    "\n",
    "amountOfRuns = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "systemprompt = \"You are an irony detector. Respond with '1' (for yes) or '0' (for no) depending on whether you think the following statements are ironic.\"#, and add a percentage value of how confident you are in your assessment.\"\n",
    "#model = \"gpt-4\" #gpt-4o, gpt-4-turbo, gpt-4, and gpt-3.5-turbo\n",
    "#model = \"gpt-4\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "for x in range(amountOfRuns):\n",
    "    resultSet = gptNoHistory(data, systemprompt, model)\n",
    "    print('Run ' + str(x + 1) + ' done!')\n",
    "    results.append(resultSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure in line 33 in gpt answer column 1 (answer format not correct). Error line: As there's no content provided, I can't detect irony.\n",
      "Failure in line 33 in gpt answer column 2 (answer format not correct). Error line: I'm sorry, I can't analyze the statement because it is missing.\n",
      "Failure in line 33 in gpt answer column 4 (answer format not correct). Error line: Apologies, but there's no statement to evaluate. Could you, please, provide a statement?\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# create dataframe that contains the original tweet (column 0), the original classification (column 1) and the gpt classification of the tweet (column 2)\n",
    "resultSetAlt = data\n",
    "runNo = 1\n",
    "resultScores = []\n",
    "\n",
    "for x in results:\n",
    "    string = 'gpt run no. ' + str(runNo)\n",
    "    runNo = runNo + 1\n",
    "    resultSetAlt[string] = x['classification']\n",
    "\n",
    "# calculate and save scores\n",
    "for x in range(amountOfRuns):\n",
    "    res = scoresBinary(resultSetAlt, x + 2)\n",
    "    resultScores.append(res)\n",
    "\n",
    "# create folder for dataset, current date and time to sort results\n",
    "now = datetime.datetime.now()\n",
    "pathTime = \"results\\\\\"+ runtype + \"\\\\\" + datasetName + '\\\\' +  str(len(resultSetAlt)) + '\\\\' + model + '\\\\' + str(now.date()) + \"_\" + str(now.time().hour) + \"-\" + str(now.time().minute)\n",
    "if not os.path.exists(pathTime):\n",
    "    os.makedirs(pathTime)\n",
    "\n",
    "# create text file containing the relevant results from the experiment\n",
    "\n",
    "linesToWrite = ['Model used: ' + model + '\\n',\n",
    "                #'Model (given by last run): ' + str(results[(amountOfRuns - 1)]['model']) + '\\n',\n",
    "                'Prompt: ' + systemprompt + '\\n',\n",
    "                'Dataset: ' + datasetPath + '\\n',\n",
    "                'Amount of individual evaluations (sample size): ' + str(len(resultSetAlt)) + '\\n\\n']\n",
    "\n",
    "resultRuns = 0\n",
    "for res in resultScores:\n",
    "    errorString = 'Errors (not parsed): \\n'\n",
    "    for error in res['error']:\n",
    "        errorString = errorString + error + '\\n'\n",
    "\n",
    "    linesToWrite = linesToWrite + ['Results for run ' + str(resultRuns + 1) + ': \\n',\n",
    "        'Matrix:' + '\\n',\n",
    "        str(res['tp']) + '  ' + str(res['fp']) + '\\n',\n",
    "        str(res['fn']) + '  ' + str(res['tn']) + '\\n',\n",
    "        'Precision: ' + str(res['precision']) + '\\n',\n",
    "        'Recall: ' + str(res['recall']) + '\\n',\n",
    "        'F1-Score: ' + str(res['F1']) + '\\n\\n',\n",
    "        errorString + '\\n\\n']\n",
    "    resultRuns = resultRuns + 1\n",
    "\n",
    "file = open(pathTime + \"\\\\metadata.txt\", \"w\")\n",
    "file.writelines(linesToWrite)\n",
    "file.close()\n",
    "\n",
    "# save the original data evaluated as well as the results (all of which is in resultSetAlt) as a csv for review if required\n",
    "resultSetAlt.to_csv(pathTime + '\\\\results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
